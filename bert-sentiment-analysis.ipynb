{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715},{"sourceId":819665,"sourceType":"datasetVersion","datasetId":431504}],"dockerImageVersionId":29981,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport torch\nimport torch.nn as nn\nimport transformers\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:56:19.175197Z","iopub.execute_input":"2025-05-07T12:56:19.175521Z","iopub.status.idle":"2025-05-07T12:56:19.180640Z","shell.execute_reply.started":"2025-05-07T12:56:19.175493Z","shell.execute_reply":"2025-05-07T12:56:19.179664Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:56:19.182652Z","iopub.execute_input":"2025-05-07T12:56:19.182992Z","iopub.status.idle":"2025-05-07T12:56:19.201140Z","shell.execute_reply.started":"2025-05-07T12:56:19.182956Z","shell.execute_reply":"2025-05-07T12:56:19.200449Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/bert-base-uncased/config.json\n/kaggle/input/bert-base-uncased/pytorch_model.bin\n/kaggle/input/bert-base-uncased/vocab.txt\n/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"class config():\n    MAX_LEN = 512\n    TRAIN_BATCH_SIZE = 8\n    VALID_BATCH_SIZE = 4\n    EPOCHS = 3\n    BERT_PATH = '../input/bert-base-uncased'\n    MODEL_PATH = 'model.bin'\n    TRAINING_FILE = \"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\n    TOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:56:19.202527Z","iopub.execute_input":"2025-05-07T12:56:19.202795Z","iopub.status.idle":"2025-05-07T12:56:19.240175Z","shell.execute_reply.started":"2025-05-07T12:56:19.202754Z","shell.execute_reply":"2025-05-07T12:56:19.239511Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class BERTBaseUncased(nn.Module):\n    def __init__(self):\n        super(BERTBaseUncased, self).__init__()\n        self.bert       = transformers.BertModel.from_pretrained(config.BERT_PATH)\n        self.bert_drop  = nn.Dropout(0.3)\n        self.out        = nn.Linear(768, 1)\n\n    def forward(self, ids, mask, token_type_ids):\n        out1, out2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n        bo         = self.bert_drop(out2)\n        output     = self.out(bo)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:56:19.241218Z","iopub.execute_input":"2025-05-07T12:56:19.241433Z","iopub.status.idle":"2025-05-07T12:56:19.247080Z","shell.execute_reply.started":"2025-05-07T12:56:19.241410Z","shell.execute_reply":"2025-05-07T12:56:19.246346Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Dataset or DataLoader","metadata":{}},{"cell_type":"code","source":"class BERTDataset:\n    def __init__(self,  review, target):\n        self.review    = review\n        self.target    = target\n        self.tokenizer = config.TOKENIZER\n        self.max_len   = config.MAX_LEN\n\n    def __len__(self):\n        return len(self.review)\n\n    def __getitem__(self, item):\n        review = str(self.review[item])\n        review = ' '.join(review.split())\n\n        inputs = self.tokenizer.encode_plus(review, None, add_special_tokens=True, max_length=self.max_len, pad_to_max_length=True)\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs['token_type_ids']\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.target[item], dtype=torch.float)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:56:19.249679Z","iopub.execute_input":"2025-05-07T12:56:19.250008Z","iopub.status.idle":"2025-05-07T12:56:19.258360Z","shell.execute_reply.started":"2025-05-07T12:56:19.249974Z","shell.execute_reply":"2025-05-07T12:56:19.257529Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## Engine","metadata":{}},{"cell_type":"code","source":"def loss_fn(outputs, targets):\n    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:56:19.260067Z","iopub.execute_input":"2025-05-07T12:56:19.260409Z","iopub.status.idle":"2025-05-07T12:56:19.271807Z","shell.execute_reply.started":"2025-05-07T12:56:19.260376Z","shell.execute_reply":"2025-05-07T12:56:19.271144Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def train_fn(data_loader, model, optimizer, device, scheduler):\n    model.train()\n\n    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n        ids = d['ids']\n        token_type_ids = d['token_type_ids']\n        mask = d['mask']\n        targets = d['targets']\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n\n        optimizer.zero_grad()\n        outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:56:19.272948Z","iopub.execute_input":"2025-05-07T12:56:19.273395Z","iopub.status.idle":"2025-05-07T12:56:19.280374Z","shell.execute_reply.started":"2025-05-07T12:56:19.273359Z","shell.execute_reply":"2025-05-07T12:56:19.279681Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def eval_fn(data_loader, model, device):\n    model.eval()\n    fin_targets = []\n    fin_outputs = []\n\n    with torch.no_grad():\n        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n            ids            = d['ids']\n            token_type_ids = d['token_type_ids']\n            mask           = d['mask']\n            targets        = d['targets']\n\n            ids            = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask           = mask.to(device, dtype=torch.long)\n            targets        = targets.to(device, dtype=torch.float)\n\n            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n\n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n\n    return fin_outputs, fin_targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:56:19.281852Z","iopub.execute_input":"2025-05-07T12:56:19.282201Z","iopub.status.idle":"2025-05-07T12:56:19.292753Z","shell.execute_reply.started":"2025-05-07T12:56:19.282149Z","shell.execute_reply":"2025-05-07T12:56:19.292245Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"dfx = pd.read_csv(config.TRAINING_FILE).fillna('none')\ndfx.sentiment = dfx.sentiment.apply(lambda x: 1 if x == 'positive' else 0)\n\ndf_temp, df_test = train_test_split(\n    dfx,\n    test_size=0.2,\n    random_state=42,\n    stratify=dfx.sentiment.values\n)\n\n# Bước 2: Tách 80% còn lại thành 60% Train và 20% Valid (tức 75/25 trong phần còn lại)\ndf_train, df_valid = train_test_split(\n    df_temp,\n    test_size=0.25,\n    random_state=42,\n    stratify=df_temp.sentiment.values\n)\n\ntest_dataset = BERTDataset(\n    review=df_test.review.values,\n    target=df_test.sentiment.values\n)\n\ntest_data_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=config.VALID_BATCH_SIZE,\n    num_workers=1\n)\n\ndf_train = df_train.reset_index(drop=True)\ndf_valid = df_valid.reset_index(drop=True)\ndf_test  = df_test.reset_index(drop=True)\n\ntrain_dataset = BERTDataset(\n    review=df_train.review.values,\n    target=df_train.sentiment.values\n)\n\ntrain_data_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=config.TRAIN_BATCH_SIZE,\n    num_workers=4 # (4) in windows gotta be 0\n)\n\nvalid_dataset = BERTDataset(\n    review=df_valid.review.values,\n    target=df_valid.sentiment.values\n)\n\nvalid_data_loader = torch.utils.data.DataLoader(\n    valid_dataset,\n    batch_size=config.VALID_BATCH_SIZE,\n    num_workers=1 # (1)\n)\n\ntest_dataset = BERTDataset(\n    review=df_test.review.values,\n    target=df_test.sentiment.values\n)\n\ntest_data_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=config.VALID_BATCH_SIZE,\n    num_workers=1\n)\n\ndevice = torch.device('cuda')\nmodel = BERTBaseUncased()\nmodel.to(device)\n\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n]\n\nnum_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\noptimizer = AdamW(optimizer_parameters, lr=3e-5)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_train_steps\n    #warmup_steps = 0, # transformers 2.1.0\n    #t_total = num_train_steps  # transformers 2.1.0\n)\n\n#model = nn.DataParallel(model)\n\nbest_acc = 0\n\nfor epoch in range(config.EPOCHS):\n    train_fn(train_data_loader, model, optimizer, device, scheduler)\n    outputs, targets = eval_fn(valid_data_loader, model, device)\n    outputs = np.array(outputs) >= 0.5\n\n    acc = metrics.accuracy_score(targets, outputs) # we can use accuracy, as the dataset is balanced\n    print('Accuracy score = ', acc)\n\n    if acc > best_acc:\n        torch.save(model.state_dict(), config.MODEL_PATH)\n        best_acc = acc\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T12:56:19.384558Z","iopub.execute_input":"2025-05-07T12:56:19.384832Z","iopub.status.idle":"2025-05-07T14:27:23.411122Z","shell.execute_reply.started":"2025-05-07T12:56:19.384810Z","shell.execute_reply":"2025-05-07T14:27:23.410206Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 3750/3750 [27:10<00:00,  2.30it/s]\n100%|██████████| 2500/2500 [03:06<00:00, 13.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy score =  0.9312\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3750/3750 [27:09<00:00,  2.30it/s]\n100%|██████████| 2500/2500 [03:06<00:00, 13.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy score =  0.9405\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3750/3750 [27:10<00:00,  2.30it/s]\n100%|██████████| 2500/2500 [03:06<00:00, 13.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Accuracy score =  0.9434\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"test_outputs, test_targets = eval_fn(test_data_loader, model, device)\ntest_outputs = np.array(test_outputs) >= 0.5\n\ntest_acc = metrics.accuracy_score(test_targets, test_outputs)\nprint(f\"Test Accuracy: {test_acc}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T14:30:44.169044Z","iopub.execute_input":"2025-05-07T14:30:44.169399Z","iopub.status.idle":"2025-05-07T14:33:50.479705Z","shell.execute_reply.started":"2025-05-07T14:30:44.169367Z","shell.execute_reply":"2025-05-07T14:33:50.478769Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 2500/2500 [03:06<00:00, 13.42it/s]","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 0.9449\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}